{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Huacos Data Set from Google Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lines follow the instructions from https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb and collect images from Google for the categories of Huacos that we are interested in. The Google search terms used were:\n",
    "\n",
    "* mochica ceramics\n",
    "* chimu ceramics\n",
    "* chavin ceramics\n",
    "* nazca ceramics\n",
    "* paracas ceramics\n",
    "* inca ceramics\n",
    "* tiahuanaco ceramics\n",
    "\n",
    "**NB** A quick visual inspection of the images shows a couple of problems: first that not all of the images returned are actual photographs (despite the fact that we asked Google to only search for photographs) and two that several of the images appeared in different searches, i.e. a the same 'huaco' appeared to be labeled as both Inca and Nazca. Some manual cleaning of the database will be needed to deal with this problem as it could affect the accuracy of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Directories for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start creating the directories for the data, the `csv` files that will point to the data and finally download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification classes. For now I am limiting the classes to four classes where I have found good data. The other classes do not results good search results and produce results that belong to the other classes. Until I find better pictures I will limite this notebook to mochica, chimu, chavin and nazca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes = ['inca', 'paracas', 'nazca', 'chimu', 'mochina', 'chavin', 'tiahuanaco']\n",
    "\n",
    "classes = ['nazca', 'chimu', 'mochina', 'chavin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donwload the images\n",
    "The next line of code has to be run once for each category. Set up the values using the cells in earlier parts of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/huacos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mochica'\n",
    "file = 'urls_mochica.csv'\n",
    "dest = path/folder\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(file)\n",
    "print(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/huacos/chimu\n"
     ]
    }
   ],
   "source": [
    "folder = 'chimu'\n",
    "file = 'urls_chimu.csv'\n",
    "dest = path/folder\n",
    "print(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='361' class='' max='361', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [361/361 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n"
     ]
    }
   ],
   "source": [
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'chavin'\n",
    "file = 'urls_chavin.csv'\n",
    "dest = path/folder\n",
    "print(dest)\n",
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'inca'\n",
    "file = 'urls_inca.csv'\n",
    "dest = path/folder\n",
    "print(dest)\n",
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paracas pottery -nazca -mochica -chimu -inca -skull -ballestas -reserve -islas -pisco filetype:jpg\n",
    "folder = 'paracas'\n",
    "file = 'urls_paracas.csv'\n",
    "dest = path/folder\n",
    "print(dest)\n",
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nazca pottery -nazca -mochica -chimu -inca -skull -ballestas -reserve -islas -pisco -paracas filetype:jp\n",
    "folder = 'nazca'\n",
    "file = 'urls_nazca.csv'\n",
    "dest = path/folder\n",
    "print(dest)\n",
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiahuanaco pottery -nazca -mochica -chimu -inca -skull -inca -ballestas -reserve -islas -pisco -paracas\n",
    "folder = 'tiahuanaco'\n",
    "file = 'urls_tiahuanaco.csv'\n",
    "dest = path/folder\n",
    "print(dest)\n",
    "download_images(path/file, dest, max_pics=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistics about the data set. Show the classes, the number of classes lenght of the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pass at training a Model\n",
    "\n",
    "Let's train a model. We will use the 'raw' data that we donwloaded. We don't expect very good results because we know that the dataset has some problems that we have already alluded to. We also noticed several items that were not picture. These will affect the results, but we need to do this so that we can clean up the data set later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very good. Let's see if we can work on this and improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix above does confirm my initial statement that the model was not doing very well. Let's try to clean up the model and remove images that should be there to see if we can improve things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, the data that was collected contains images that should not be in the dataset or that should not be in a particular dataset. These 'rogue' images will impact the accuracy and performance of the model. So we are going to clean the data set using the ImageCleaner widget from fastai.widgets . With this widget we can prune our top losses, removing photos that don't belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.widgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the file paths from our top_losses. We can do this with .from_toplosses. We then feed the top losses indexes and corresponding dataset to ImageCleaner. In order to clean the entire set of images, we need to create a new dataset without the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = (ImageList.from_folder(path)\n",
    "                   .split_none()\n",
    "                   .label_from_folder()\n",
    "                   .transform(get_transforms(), size=224)\n",
    "                   .databunch()\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new learner to use our new databunch with all of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)\n",
    "\n",
    "learn_cln.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, idxs = DatasetFormatter().from_toplosses(learn_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the image cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageCleaner(ds, idxs, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Cleaned up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you already cleaned your data, run this cell instead of the one before\n",
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_csv(path, folder=\".\", valid_pct=0.2, csv_labels='cleaned.csv',\n",
    "         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already cleaned up the data from `top_losses` we run a different set of commands to create `db` and the new data bunch. Otherwise all results will be overwritten by the new run of `ImageCleaner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = (ImageList.from_csv(path, 'cleaned.csv', folder='.')\n",
    "                    .split_none()\n",
    "                    .label_from_df()\n",
    "                    .transform(get_transforms(), size=224)\n",
    "                    .databunch()\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds, idxs = DatasetFormatter().from_similars(learn_cln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ImageCleaner(ds, idxs, path, duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the databunch\n",
    "#db = (ImageList.from_csv(path, 'cleaned.csv', folder='.')\n",
    "#                    .split_none()\n",
    "#                    .label_from_df()\n",
    "#                    .transform(get_transforms(), size=224)\n",
    "#                    .databunch()\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New learner\n",
    "#learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.save('stage-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_cln.fit_one_cycle(16, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn_cln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn_cln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
